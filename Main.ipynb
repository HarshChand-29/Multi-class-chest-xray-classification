{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c9d05c-f7a0-4528-876e-7f62b51b8eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\MajorPro\\Lib\\site-packages\\torch\\__init__.py:262\u001b[39m\n\u001b[32m    258\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    260\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     _load_dll_libraries()\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\MajorPro\\Lib\\site-packages\\torch\\__init__.py:238\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    236\u001b[39m is_loaded = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     res = kernel32.LoadLibraryExW(dll, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0x00001100\u001b[39m)\n\u001b[32m    239\u001b[39m     last_error = ctypes.get_last_error()\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error != \u001b[32m126\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup, paths, seeds\n",
    "import os, random, pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, recall_score, roc_curve, auc\n",
    "\n",
    "# ---------------- Paths (your provided paths) ----------------\n",
    "csv_path = r\" ## Path to Csv ##\"\n",
    "images_dir = r\" ##path to Images directory##\"\n",
    "results_dir = r\"##path to the folder you want to save your results##\"\n",
    "\n",
    "# make subfolders\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "for sf in [\"checkpoints\", \"pickles\", \"reports\", \"plots\"]:\n",
    "    os.makedirs(os.path.join(results_dir, sf), exist_ok=True)\n",
    "\n",
    "# reproducibility + device\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Results folder:\", results_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da100fb5-edf4-4df1-b170-ce75bbd69618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load CSV & inspect\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Total rows in CSV:\", len(df))\n",
    "print(\"Categories and counts:\")\n",
    "print(df['Category'].value_counts())\n",
    "\n",
    "# Ensure 'Image Index' column exists (NIH style). If different, change below.\n",
    "if \"Image Index\" not in df.columns and \"path\" in df.columns:\n",
    "    print(\"Using 'path' column for image filenames.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc198910-5f63-40dd-9f0d-62f47f288eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Dataset class (CLAHE applied) and train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset with CLAHE on grayscale -> convert to RGB\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, images_root, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.images_root = images_root\n",
    "        self.transform = transform\n",
    "        self.label_col = \"Category\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx][\"Image Index\"] if \"Image Index\" in self.df.columns else self.df.iloc[idx][\"path\"]\n",
    "        img_path = os.path.join(self.images_root, img_name)\n",
    "        # read grayscale, apply CLAHE\n",
    "        img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img_gray is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        img_clahe = clahe.apply(img_gray)\n",
    "        img_rgb = cv2.cvtColor(img_clahe, cv2.COLOR_GRAY2RGB)\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "\n",
    "        label = self.df.iloc[idx][self.label_col]\n",
    "        return (self.transform(img_pil) if self.transform else img_pil, label)\n",
    "\n",
    "# stratified split: train 80%, val 10%, test 10%\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['Category'], random_state=seed)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['Category'], random_state=seed)\n",
    "print(\"Split sizes -> train:\", len(train_df), \"val:\", len(val_df), \"test:\", len(test_df))\n",
    "\n",
    "# transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Datasets & loaders\n",
    "batch_size = 32\n",
    "num_workers = 0 if torch.cuda.is_available() else 0\n",
    "\n",
    "train_dataset = ChestXrayDataset(train_df, images_dir, transform=train_transforms)\n",
    "val_dataset   = ChestXrayDataset(val_df, images_dir, transform=val_transforms)\n",
    "test_dataset  = ChestXrayDataset(test_df, images_dir, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862de361-970f-40f1-9a94-57cd775dc637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: class name -> index mapping (consistent across notebook)\n",
    "classes = sorted(df['Category'].unique())  # e.g. ['Effusion','Infiltration','No Finding']\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "idx_to_class = {i:c for c,i in class_to_idx.items()}\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Mapping:\", class_to_idx)\n",
    "\n",
    "# Replace labels in datasets with numeric indices using a wrapper dataset\n",
    "class MapLabelDataset(Dataset):\n",
    "    def __init__(self, ds, mapping):\n",
    "        self.ds = ds\n",
    "        self.mapping = mapping\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.ds[idx]\n",
    "        return img, self.mapping[label]\n",
    "\n",
    "train_loader = DataLoader(MapLabelDataset(train_dataset, class_to_idx), batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader   = DataLoader(MapLabelDataset(val_dataset, class_to_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader  = DataLoader(MapLabelDataset(test_dataset, class_to_idx), batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317334b-df8f-4a8e-921d-2dcde9b0d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from transformers import ViTModel\n",
    "\n",
    "class CNNViTHybrid(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CNNViTHybrid, self).__init__()\n",
    "        \n",
    "        # --- DenseNet121 Backbone ---\n",
    "        self.cnn = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        self.cnn.classifier = nn.Identity()  # remove classifier to get features\n",
    "        \n",
    "        # --- ViT Backbone ---\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        \n",
    "        # --- Fusion Layer ---\n",
    "        cnn_feat_dim = 1024\n",
    "        vit_feat_dim = self.vit.config.hidden_size  # 768\n",
    "        fusion_dim = cnn_feat_dim + vit_feat_dim\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # DenseNet feature extraction\n",
    "        cnn_features = self.cnn.features(x)\n",
    "        cnn_features = nn.functional.adaptive_avg_pool2d(cnn_features, (1,1))\n",
    "        cnn_features = torch.flatten(cnn_features, 1)\n",
    "        \n",
    "        # ViT expects (batch, channels, height, width)\n",
    "        vit_out = self.vit(x)\n",
    "        vit_features = vit_out.pooler_output\n",
    "        \n",
    "        # Concatenate CNN and ViT features\n",
    "        combined = torch.cat((cnn_features, vit_features), dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f94e54-29c0-4acc-9524-090ada8741f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNViTHybrid(num_classes=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
    "\n",
    "ckpt_dir = os.path.join(results_dir, \"checkpoints\")\n",
    "best_ckpt = os.path.join(ckpt_dir, \"hybrid_best.pth\")# Cell 5: Hybrid model (DenseNet121 encoder -> transformer encoder -> head)\n",
    "class CNNViTHybrid(nn.Module):\n",
    "    def __init__(self, num_classes=3, cnn_out_channels=1024, embed_dim=384, depth=6, num_heads=6, mlp_ratio=4.0, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        self.cnn = densenet.features                 # output expected [B,1024,7,7]\n",
    "        self.cnn_norm = nn.BatchNorm2d(cnn_out_channels)\n",
    "        self.proj = nn.Linear(cnn_out_channels, embed_dim)\n",
    "\n",
    "        self.grid_size = 7\n",
    "        num_patches = self.grid_size * self.grid_size\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1,1+num_patches,embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=int(embed_dim*mlp_ratio),\n",
    "            dropout=drop_rate,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.cnn_norm(x)\n",
    "        B,C,H,W = x.shape\n",
    "        x = x.flatten(2).transpose(1,2)   # [B,49,C]\n",
    "        x = self.proj(x)                  # [B,49,embed_dim]\n",
    "        cls = self.cls_token.expand(B,-1,-1)\n",
    "        x = torch.cat([cls, x], dim=1)    # [B,1+49,embed_dim]\n",
    "        x = x + self.pos_embed\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x[:,0])\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# instantiate\n",
    "model = CNNViTHybrid(num_classes=len(classes)).to(device)\n",
    "print(\"Model instantiated, output classes:\", len(classes))\n",
    "\n",
    "latest_ckpt = os.path.join(ckpt_dir, \"hybrid_latest.pth\")\n",
    "pickle_path = os.path.join(results_dir, \"pickles\", \"hybrid_3class.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d24a2-c1d7-45e5-a86c-2691f056418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Loss, optimizer, scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "# We'll use ReduceLROnPlateau with mode='max' because we monitor val metric (higher is better)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
    "\n",
    "best_metric = 0.0   # best of (val_f1, val_recall)\n",
    "patience = 6\n",
    "no_improve = 0\n",
    "num_epochs = 30\n",
    "\n",
    "best_state_path = os.path.join(results_dir, \"checkpoints\", \"stage1_best_state.pth\")\n",
    "best_pickle_path = os.path.join(results_dir, \"pickles\", \"hybrid_stage1.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427ca69-7d03-4b0e-9edb-d0ad11f076f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 7: Training loop (monitor max(val_f1, val_recall_macro))\n",
    "from collections import defaultdict\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loop = tqdm(train_loader, desc=f\"[Stage1] Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds==labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            preds = outputs.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    val_recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    val_metric = max(val_f1, val_recall)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, val_f1={val_f1:.4f}, val_recall={val_recall:.4f}, monitor_metric={val_metric:.4f}\")\n",
    "\n",
    "    # scheduler step (pass the monitored metric)\n",
    "    scheduler.step(val_metric)\n",
    "\n",
    "    # early stopping & save best\n",
    "    if val_metric > best_metric:\n",
    "        best_metric = val_metric\n",
    "        no_improve = 0\n",
    "        # save state_dict and full pickle\n",
    "        torch.save(model.state_dict(), best_state_path)\n",
    "        torch.save(model, best_pickle_path)\n",
    "        # save classification report for this epoch\n",
    "        report = classification_report(all_labels, all_preds, target_names=classes)\n",
    "        with open(os.path.join(results_dir, \"reports\", f\"stage1_report_epoch{epoch+1}.txt\"), \"w\") as f:\n",
    "            f.write(report)\n",
    "        print(\"üèÜ New best model saved (state + pickle).\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"‚ö†Ô∏è No improvement count: {no_improve}/{patience}\")\n",
    "        if no_improve >= patience:\n",
    "            print(\"‚èπ Early stopping triggered (Stage1).\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78745171-7eed-40fc-a310-0ce39789ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Stage1 Test evaluation & save results\n",
    "# load best\n",
    "state = torch.load(best_state_path, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.to(device); model.eval()\n",
    "\n",
    "all_preds, all_labels, all_probs = [], [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_loader, desc=\"Stage1 Testing\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = outputs.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy()); all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# classification report\n",
    "report = classification_report(all_labels, all_preds, target_names=classes)\n",
    "print(report)\n",
    "with open(os.path.join(results_dir, \"reports\", \"stage1_test_report.txt\"), \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,5)); sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.title(\"Stage1 Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.savefig(os.path.join(results_dir, \"plots\", \"stage1_confusion_matrix.png\")); plt.show(); plt.close()\n",
    "\n",
    "# ROC curves (one-vs-rest)\n",
    "plt.figure(figsize=(7,6))\n",
    "all_labels_bin = torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=len(classes)).numpy()\n",
    "for i, cls in enumerate(classes):\n",
    "    fpr, tpr, _ = roc_curve(all_labels_bin[:, i], [p[i] for p in all_probs])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{cls} (AUC={roc_auc:.2f})\", lw=2)\n",
    "plt.plot([0,1],[0,1],\"--\", color=\"gray\"); plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Stage1 ROC Curves\"); plt.savefig(os.path.join(results_dir, \"plots\", \"stage1_roc.png\")); plt.show(); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b5ab1-677b-452a-9cfe-92231ae73af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Binary dataset (Infiltration vs No Finding)\n",
    "binary_classes = [\"Infiltration\", \"No Finding\"]\n",
    "train_bin_df = train_df[train_df['Category'].isin(binary_classes)].reset_index(drop=True)\n",
    "val_bin_df   = val_df[val_df['Category'].isin(binary_classes)].reset_index(drop=True)\n",
    "test_bin_df  = test_df[test_df['Category'].isin(binary_classes)].reset_index(drop=True)\n",
    "\n",
    "binary_map = {binary_classes[0]: 0, binary_classes[1]: 1}\n",
    "\n",
    "class BinaryDataset(Dataset):\n",
    "    def __init__(self, dataframe, images_root, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.images_root = images_root\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx][\"Image Index\"] if \"Image Index\" in self.df.columns else self.df.iloc[idx][\"path\"]\n",
    "        img_path = os.path.join(self.images_root, img_name)\n",
    "        img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        img_clahe = clahe.apply(img_gray)\n",
    "        img_rgb = cv2.cvtColor(img_clahe, cv2.COLOR_GRAY2RGB)\n",
    "        img_pil = Image.fromarray(img_rgb)\n",
    "        label = self.df.iloc[idx][\"Category\"]\n",
    "        return (self.transform(img_pil) if self.transform else img_pil, binary_map[label])\n",
    "\n",
    "train_bin_loader = DataLoader(BinaryDataset(train_bin_df, images_dir, transform=train_transforms),\n",
    "                              batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_bin_loader   = DataLoader(BinaryDataset(val_bin_df, images_dir, transform=val_transforms),\n",
    "                              batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_bin_loader  = DataLoader(BinaryDataset(test_bin_df, images_dir, transform=val_transforms),\n",
    "                              batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(\"Binary set sizes -> train:\", len(train_bin_df), \"val:\", len(val_bin_df), \"test:\", len(test_bin_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28373936-b1de-454b-aa2e-ba509f082e9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 10: Binary model (reuse features from stage1)\n",
    "binary_model = CNNViTHybrid(num_classes=2).to(device)\n",
    "\n",
    "# load stage1 state dict and copy shared weights except head\n",
    "stage1_state = torch.load(best_state_path, map_location=device) if os.path.exists(best_state_path) else None\n",
    "if stage1_state is not None:\n",
    "    # stage1_state is state_dict saved earlier\n",
    "    filtered = {k: v for k, v in stage1_state.items() if not k.startswith(\"head\")}\n",
    "    missing, unexpected = binary_model.load_state_dict(filtered, strict=False)\n",
    "    print(\"Loaded shared weights from Stage1 into binary model. Missing/unexpected keys:\", missing, unexpected)\n",
    "else:\n",
    "    print(\"No Stage1 weights found, training binary model from scratch.\")\n",
    "\n",
    "criterion_b = nn.CrossEntropyLoss()\n",
    "optimizer_b = optim.AdamW(binary_model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "scheduler_b = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_b, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "best_bin_metric = 0.0\n",
    "patience_bin = 6\n",
    "no_imp_bin = 0\n",
    "best_bin_state = os.path.join(results_dir, \"checkpoints\", \"stage2_best_state.pth\")\n",
    "best_bin_pickle = os.path.join(results_dir, \"pickles\", \"hybrid_stage2_binary.pkl\")\n",
    "\n",
    "num_epochs_bin = 20\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "for epoch in range(num_epochs_bin):\n",
    "    binary_model.train()\n",
    "    for imgs, labels in tqdm(train_bin_loader, desc=f\"[Stage2] Epoch {epoch+1}/{num_epochs_bin}\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer_b.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")):\n",
    "            outs = binary_model(imgs)\n",
    "            loss = criterion_b(outs, labels)\n",
    "        loss.backward(); optimizer_b.step()\n",
    "\n",
    "    # validation\n",
    "    binary_model.eval()\n",
    "    all_p, all_l = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_bin_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds = binary_model(imgs).argmax(1)\n",
    "            all_p.extend(preds.cpu().numpy()); all_l.extend(labels.cpu().numpy())\n",
    "    val_f1_b = f1_score(all_l, all_p, average='macro')\n",
    "    val_recall_b = recall_score(all_l, all_p, average='macro')\n",
    "    val_metric_b = max(val_f1_b, val_recall_b)\n",
    "    print(f\"Stage2 Epoch {epoch+1}: val_f1={val_f1_b:.4f}, val_recall={val_recall_b:.4f}, monitor_metric={val_metric_b:.4f}\")\n",
    "    scheduler_b.step(val_metric_b)\n",
    "\n",
    "    if val_metric_b > best_bin_metric:\n",
    "        best_bin_metric = val_metric_b\n",
    "        no_imp_bin = 0\n",
    "        torch.save(binary_model.state_dict(), best_bin_state)\n",
    "        torch.save(binary_model, best_bin_pickle)\n",
    "        # save report\n",
    "        report = classification_report(all_l, all_p, target_names=binary_classes)\n",
    "        with open(os.path.join(results_dir, \"reports\", f\"stage2_report_epoch{epoch+1}.txt\"), \"w\") as f:\n",
    "            f.write(report)\n",
    "        print(\"üèÜ New best binary model saved.\")\n",
    "    else:\n",
    "        no_imp_bin += 1\n",
    "        print(f\"‚ö†Ô∏è No improvement count (binary): {no_imp_bin}/{patience_bin}\")\n",
    "        if no_imp_bin >= patience_bin:\n",
    "            print(\"‚èπ Early stopping triggered (Stage2).\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7573d5c-ec5d-422d-9e38-70823e38cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Binary test evaluation\n",
    "binary_model.load_state_dict(torch.load(best_bin_state, map_location=device))\n",
    "binary_model.to(device); binary_model.eval()\n",
    "\n",
    "all_p, all_l, all_pr = [], [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(test_bin_loader, desc=\"Binary Testing\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outs = binary_model(imgs)\n",
    "        probs = torch.softmax(outs, dim=1)\n",
    "        preds = outs.argmax(1)\n",
    "        all_p.extend(preds.cpu().numpy()); all_l.extend(labels.cpu().numpy()); all_pr.extend(probs.cpu().numpy())\n",
    "\n",
    "report_bin = classification_report(all_l, all_p, target_names=binary_classes)\n",
    "print(report_bin)\n",
    "with open(os.path.join(results_dir, \"reports\", \"stage2_test_report.txt\"), \"w\") as f:\n",
    "    f.write(report_bin)\n",
    "\n",
    "# confusion\n",
    "cm = confusion_matrix(all_l, all_p)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=binary_classes, yticklabels=binary_classes)\n",
    "plt.title(\"Stage2 Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.savefig(os.path.join(results_dir, \"plots\", \"stage2_confusion_matrix.png\")); plt.show(); plt.close()\n",
    "\n",
    "# ROC\n",
    "plt.figure(figsize=(6,5))\n",
    "all_l_bin_oh = torch.nn.functional.one_hot(torch.tensor(all_l), num_classes=2).numpy()\n",
    "for i, cls in enumerate(binary_classes):\n",
    "    fpr, tpr, _ = roc_curve(all_l_bin_oh[:, i], [p[i] for p in all_pr])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{cls} (AUC={roc_auc:.2f})\", lw=2)\n",
    "plt.plot([0,1],[0,1],\"--\", color=\"gray\"); plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Stage2 ROC\"); plt.savefig(os.path.join(results_dir, \"plots\", \"stage2_roc.png\")); plt.show(); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54547bdc-0650-42ca-92e1-dbcf1dea9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Hierarchical inference function that loads pickles and runs stage1 then stage2\n",
    "def hierarchical_predict(img_tensor_batch, stage1_model, stage2_model, threshold=None):\n",
    "    \"\"\"\n",
    "    img_tensor_batch: batch of preprocessed tensors on device\n",
    "    stage1_model: PyTorch model (3-class)\n",
    "    stage2_model: PyTorch model (binary)\n",
    "    Returns predicted class indices (relative to 'classes' list)\n",
    "    \"\"\"\n",
    "    stage1_model.eval(); stage2_model.eval()\n",
    "    with torch.no_grad():\n",
    "        out1 = stage1_model(img_tensor_batch)\n",
    "        p1 = torch.softmax(out1, dim=1)\n",
    "        pred1 = p1.argmax(1).cpu().numpy()\n",
    "\n",
    "        final_preds = []\n",
    "        for i in range(len(pred1)):\n",
    "            cls_idx = pred1[i]\n",
    "            cls_name = classes[cls_idx]\n",
    "            if cls_name == \"Effusion\":\n",
    "                final_preds.append(class_idx := cls_idx)\n",
    "            else:\n",
    "                # pass image through stage2\n",
    "                img = img_tensor_batch[i].unsqueeze(0)\n",
    "                out2 = stage2_model(img)\n",
    "                pred2 = out2.argmax(1).item()\n",
    "                # map binary index to overall class idx\n",
    "                mapped = class_to_idx[binary_classes[pred2]]\n",
    "                final_preds.append(mapped)\n",
    "        return final_preds\n",
    "\n",
    "# Example: run hierarchical on the full test_loader and print final classification report\n",
    "# load models from pickles (safer for exact architecture)\n",
    "stage1 = torch.load(os.path.join(results_dir, \"pickles\", \"hybrid_stage1.pkl\"), map_location=device)\n",
    "stage2 = torch.load(os.path.join(results_dir, \"pickles\", \"hybrid_stage2_binary.pkl\"), map_location=device)\n",
    "stage1.to(device); stage2.to(device)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "for imgs, labels in tqdm(test_loader, desc=\"Hierarchical Testing\"):\n",
    "    imgs = imgs.to(device)\n",
    "    preds = hierarchical_predict(imgs, stage1, stage2)\n",
    "    y_pred.extend(preds)\n",
    "    y_true.extend(labels.numpy())\n",
    "\n",
    "print(\"Hierarchical final report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97c255-44e5-46c7-9a10-9949d3f0dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where you want to save the CSV files (adjust path as needed)\n",
    "split_dir = r\"C:\\Users\\harsh\\OneDrive\\Documents\\main project\\balanced ds\\splits\" # Example: create a 'splits' subfolder\n",
    "os.makedirs(split_dir, exist_ok=True) # Create the directory if it doesn't exist\n",
    "\n",
    "# Construct full file paths\n",
    "train_csv_path = os.path.join(split_dir, \"train_split.csv\")\n",
    "val_csv_path = os.path.join(split_dir, \"val_split.csv\")\n",
    "test_csv_path = os.path.join(split_dir, \"test_split.csv\")\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "# index=False prevents pandas from writing the DataFrame index as a column\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "test_df.to_csv(test_csv_path, index=False)\n",
    "\n",
    "print(f\"Train split saved to: {train_csv_path}\")\n",
    "print(f\"Validation split saved to: {val_csv_path}\")\n",
    "print(f\"Test split saved to: {test_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf52a8-e61d-406b-ad35-2d61386345b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
